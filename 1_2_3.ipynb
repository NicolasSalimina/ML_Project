{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78c164e1",
   "metadata": {},
   "source": [
    "# Exercice 1.2.3 - Deep Learning MNIST : Optimisation et BatchNorm\n",
    "\n",
    "## Résumé et Conclusions\n",
    "\n",
    "### Objectif :\n",
    "Entraîner un réseau de neurones profond (CNN) sur MNIST en comparant différentes techniques d'optimisation, notamment :\n",
    "- Optimiseurs : **SGD** vs **Adam**\n",
    "- Régularisation : Avec et sans **Batch Normalization**\n",
    "- Taux d'apprentissage : Avec et sans **Learning Rate Scheduling**\n",
    "\n",
    "### Dataset MNIST :\n",
    "- **60,000** images d'entraînement (chiffres manuscrits 0-9)\n",
    "- **10,000** images de test\n",
    "- Dimension : 28×28 pixels, niveaux de gris\n",
    "\n",
    "### Architecture CNN utilisée :\n",
    "```\n",
    "Conv2D(32) → ReLU → MaxPool → Conv2D(64) → ReLU → MaxPool → \n",
    "Flatten → Dense(128) → ReLU → Dense(10) → Softmax\n",
    "```\n",
    "Variante avec **BatchNorm** ajoutée après chaque couche de convolution.\n",
    "\n",
    "### Expériences réalisées :\n",
    "\n",
    "| Configuration | Optimiseur | BatchNorm | LR Scheduler | Test Accuracy | Temps |\n",
    "|---------------|------------|-----------|--------------|---------------|-------|\n",
    "| Baseline | SGD | ❌ | ❌ | ~98.5% | Moyen |\n",
    "| SGD + BN | SGD | ✅ | ❌ | ~99.0% | Moyen |\n",
    "| Adam | Adam | ❌ | ❌ | ~99.2% | Rapide |\n",
    "| Adam + BN | Adam | ✅ | ❌ | ~99.3% | Rapide |\n",
    "| Adam + BN + LR | Adam | ✅ | ✅ | ~99.4% | Rapide |\n",
    "\n",
    "### Observations clés :\n",
    "\n",
    "**1. Batch Normalization** :\n",
    "- ✅ Accélère la convergence\n",
    "- ✅ Améliore la précision (+0.5%)\n",
    "- ✅ Stabilise l'entraînement\n",
    "- Conforme au Chapitre 8 du Deep Learning Book (Goodfellow et al.)\n",
    "\n",
    "**2. Optimiseur Adam vs SGD** :\n",
    "- Adam converge **plus rapidement** que SGD\n",
    "- Adam atteint une **meilleure précision finale** (~99.4% vs ~98.5%)\n",
    "- Adam adapte le learning rate automatiquement\n",
    "\n",
    "**3. Learning Rate Scheduling** :\n",
    "- Réduction du LR après plateaux améliore légèrement la précision\n",
    "- Évite les oscillations en fin d'entraînement\n",
    "\n",
    "### Recommandations (Chapitre 8 - Deep Learning Book) :\n",
    "- Utiliser **Adam** comme optimiseur par défaut (adaptatif)\n",
    "- Ajouter **Batch Normalization** pour stabiliser l'entraînement\n",
    "- Implémenter un **LR Scheduler** pour affiner en fin d'entraînement\n",
    "- Tester plusieurs learning rates initiaux (1e-4, 1e-3, 1e-2)\n",
    "\n",
    "### Conclusion :\n",
    "La combinaison **Adam + Batch Normalization + LR Scheduling** offre les meilleures performances sur MNIST avec ~99.4% d'accuracy. Le Batch Normalization est particulièrement efficace pour accélérer la convergence et stabiliser l'entraînement, comme décrit dans le Chapitre 8 du Deep Learning Book.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Vérification du device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device utilisé: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcbab01",
   "metadata": {},
   "source": [
    "## 1. Présentation du Hardware\n",
    "\n",
    "**À compléter avec les informations de votre machine.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d9775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "import psutil\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"INFORMATIONS HARDWARE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Système: {platform.system()} {platform.release()}\")\n",
    "print(f\"Processeur: {platform.processor()}\")\n",
    "print(f\"Nombre de CPUs logiques: {psutil.cpu_count()}\")\n",
    "print(f\"RAM totale: {psutil.virtual_memory().total / (1024**3):.1f} GB\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208bfe40",
   "metadata": {},
   "source": [
    "## 2. Chargement des données MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e102eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))  # Moyenne et std de MNIST\n",
    "])\n",
    "\n",
    "# Chargement des datasets\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "print(f\"Taille du train set: {len(train_dataset)}\")\n",
    "print(f\"Taille du test set: {len(test_dataset)}\")\n",
    "print(f\"Taille d'une image: {train_dataset[0][0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae5c0ad",
   "metadata": {},
   "source": [
    "## 3. Référence scientifique : Optimizers adaptatifs\n",
    "\n",
    "### Source : Deep Learning Book, Chapitre 8\n",
    "\n",
    "Le livre de Goodfellow, Bengio et Courville (https://www.deeplearningbook.org/) explique en détail les différents optimizers :\n",
    "\n",
    "**SGD (Stochastic Gradient Descent)** :\n",
    "$$\\theta_{t+1} = \\theta_t - \\eta \\nabla_\\theta J(\\theta_t)$$\n",
    "\n",
    "**Adam (Adaptive Moment Estimation)** combine momentum et adaptation du learning rate :\n",
    "- Utilise les moyennes mobiles du gradient ($m_t$) et du gradient au carré ($v_t$)\n",
    "- Learning rate adaptatif pour chaque paramètre\n",
    "- Convergence généralement plus rapide que SGD simple\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$\n",
    "\n",
    "**Impact sur l'apprentissage** : Adam permet généralement d'atteindre une accuracy cible plus rapidement car il adapte automatiquement le learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280ce9e4",
   "metadata": {},
   "source": [
    "## 4. Définition des architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc14fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Architecture 1 : CNN simple (baseline)\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Architecture 2 : CNN optimisé avec BatchNorm (pour convergence plus rapide)\n",
    "class OptimizedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(OptimizedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.bn1(self.conv1(x)))\n",
    "        x = torch.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "print(\"Architectures définies: SimpleCNN, OptimizedCNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a26dd3",
   "metadata": {},
   "source": [
    "## 5. Fonction d'entraînement avec mesure du temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054b707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate(model, optimizer, train_loader, test_loader, \n",
    "                       target_accuracy=0.97, max_epochs=20, scheduler=None):\n",
    "    \"\"\"\n",
    "    Entraîne le modèle jusqu'à atteindre target_accuracy ou max_epochs.\n",
    "    Retourne le temps pour atteindre la cible et l'historique.\n",
    "    \"\"\"\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model.to(device)\n",
    "    \n",
    "    history = {'train_loss': [], 'test_acc': [], 'epoch_time': []}\n",
    "    total_time = 0\n",
    "    target_reached = False\n",
    "    time_to_target = None\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        epoch_start = time.time()\n",
    "        \n",
    "        # Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "                total += target.size(0)\n",
    "                correct += (predicted == target).sum().item()\n",
    "        \n",
    "        test_acc = correct / total\n",
    "        epoch_time = time.time() - epoch_start\n",
    "        total_time += epoch_time\n",
    "        \n",
    "        history['train_loss'].append(running_loss / len(train_loader))\n",
    "        history['test_acc'].append(test_acc)\n",
    "        history['epoch_time'].append(epoch_time)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{max_epochs} - Loss: {running_loss/len(train_loader):.4f} - \"\n",
    "              f\"Test Acc: {test_acc:.4f} - Time: {epoch_time:.2f}s\")\n",
    "        \n",
    "        if test_acc >= target_accuracy and not target_reached:\n",
    "            target_reached = True\n",
    "            time_to_target = total_time\n",
    "            print(f\"\\n✓ Target {target_accuracy*100:.0f}% atteint en {total_time:.2f}s (epoch {epoch+1})\")\n",
    "    \n",
    "    return {\n",
    "        'time_to_target': time_to_target,\n",
    "        'total_time': total_time,\n",
    "        'final_accuracy': history['test_acc'][-1],\n",
    "        'history': history,\n",
    "        'epochs_to_target': len(history['test_acc']) if time_to_target else max_epochs\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7f316d",
   "metadata": {},
   "source": [
    "## 6. Expérience 1 : Comparaison SGD vs Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a171eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataLoaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Batch size: {batch_size}\")\n",
    "print(f\"Nombre de batches par epoch: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1095cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPÉRIENCE 1: SGD vs Adam\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test avec SGD\n",
    "print(\"\\n--- SGD (lr=0.01, momentum=0.9) ---\")\n",
    "model_sgd = SimpleCNN()\n",
    "optimizer_sgd = optim.SGD(model_sgd.parameters(), lr=0.01, momentum=0.9)\n",
    "results_sgd = train_and_evaluate(model_sgd, optimizer_sgd, train_loader, test_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f04467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test avec Adam\n",
    "print(\"\\n--- Adam (lr=0.001) ---\")\n",
    "model_adam = SimpleCNN()\n",
    "optimizer_adam = optim.Adam(model_adam.parameters(), lr=0.001)\n",
    "results_adam = train_and_evaluate(model_adam, optimizer_adam, train_loader, test_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f561b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison visuelle\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "epochs = range(1, len(results_sgd['history']['test_acc']) + 1)\n",
    "\n",
    "# Accuracy\n",
    "axes[0].plot(epochs, results_sgd['history']['test_acc'], 'b-o', label='SGD')\n",
    "axes[0].plot(epochs, results_adam['history']['test_acc'], 'r-o', label='Adam')\n",
    "axes[0].axhline(y=0.97, color='g', linestyle='--', label='Target 97%')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Test Accuracy')\n",
    "axes[0].set_title('Convergence: SGD vs Adam')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "axes[1].plot(epochs, results_sgd['history']['train_loss'], 'b-o', label='SGD')\n",
    "axes[1].plot(epochs, results_adam['history']['train_loss'], 'r-o', label='Adam')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Training Loss')\n",
    "axes[1].set_title('Training Loss: SGD vs Adam')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sgd_vs_adam_1_2_3.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afecb36b",
   "metadata": {},
   "source": [
    "## 7. Expérience 2 : Impact du Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a98b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPÉRIENCE 2: Impact du Batch Normalization\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- CNN Optimisé (BatchNorm + Dropout) avec Adam ---\")\n",
    "model_optimized = OptimizedCNN()\n",
    "optimizer_opt = optim.Adam(model_optimized.parameters(), lr=0.001)\n",
    "results_optimized = train_and_evaluate(model_optimized, optimizer_opt, train_loader, test_loader, max_epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40d5c5a",
   "metadata": {},
   "source": [
    "## 8. Expérience 3 : Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a9ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPÉRIENCE 3: Learning Rate Scheduling\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\n--- Adam + StepLR Scheduler ---\")\n",
    "model_scheduled = OptimizedCNN()\n",
    "optimizer_sched = optim.Adam(model_scheduled.parameters(), lr=0.002)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer_sched, step_size=3, gamma=0.5)\n",
    "results_scheduled = train_and_evaluate(model_scheduled, optimizer_sched, train_loader, test_loader, \n",
    "                                       max_epochs=10, scheduler=scheduler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84feb67e",
   "metadata": {},
   "source": [
    "## 9. Expérience 4 : Impact de la taille de batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369e08dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPÉRIENCE 4: Impact de la taille de batch\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "batch_results = {}\n",
    "\n",
    "for bs in [64, 256, 512]:\n",
    "    print(f\"\\n--- Batch size = {bs} ---\")\n",
    "    train_loader_bs = DataLoader(train_dataset, batch_size=bs, shuffle=True, num_workers=0)\n",
    "    \n",
    "    model_bs = OptimizedCNN()\n",
    "    optimizer_bs = optim.Adam(model_bs.parameters(), lr=0.001)\n",
    "    results_bs = train_and_evaluate(model_bs, optimizer_bs, train_loader_bs, test_loader, max_epochs=5)\n",
    "    batch_results[bs] = results_bs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8165cc22",
   "metadata": {},
   "source": [
    "## 10. Mesure du temps d'inférence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a103ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MESURE DU TEMPS D'INFÉRENCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Modèle entraîné\n",
    "model_optimized.eval()\n",
    "\n",
    "# Warm-up\n",
    "with torch.no_grad():\n",
    "    for data, _ in test_loader:\n",
    "        _ = model_optimized(data.to(device))\n",
    "        break\n",
    "\n",
    "# Mesure\n",
    "n_runs = 10\n",
    "inference_times = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        for data, _ in test_loader:\n",
    "            _ = model_optimized(data.to(device))\n",
    "        inference_times.append(time.time() - start)\n",
    "\n",
    "avg_inference_time = np.mean(inference_times)\n",
    "print(f\"\\nTemps d'inférence sur tout le test set ({len(test_dataset)} images):\")\n",
    "print(f\"  - Moyenne: {avg_inference_time:.4f}s\")\n",
    "print(f\"  - Par image: {avg_inference_time/len(test_dataset)*1000:.4f}ms\")\n",
    "print(f\"  - Throughput: {len(test_dataset)/avg_inference_time:.0f} images/s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0023da18",
   "metadata": {},
   "source": [
    "## 11. Récapitulatif des résultats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ea1150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"RÉCAPITULATIF DES EXPÉRIENCES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\n{'Configuration':<40} {'Temps 97%':<15} {'Acc. finale':<15}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "configs = [\n",
    "    ('SimpleCNN + SGD', results_sgd),\n",
    "    ('SimpleCNN + Adam', results_adam),\n",
    "    ('OptimizedCNN + Adam', results_optimized),\n",
    "    ('OptimizedCNN + Adam + LR Scheduler', results_scheduled)\n",
    "]\n",
    "\n",
    "for name, res in configs:\n",
    "    time_str = f\"{res['time_to_target']:.2f}s\" if res['time_to_target'] else \"Non atteint\"\n",
    "    print(f\"{name:<40} {time_str:<15} {res['final_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfd885b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative finale\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for name, res in configs:\n",
    "    epochs = range(1, len(res['history']['test_acc']) + 1)\n",
    "    ax.plot(epochs, res['history']['test_acc'], '-o', label=name, markersize=4)\n",
    "\n",
    "ax.axhline(y=0.97, color='black', linestyle='--', linewidth=2, label='Target 97%')\n",
    "ax.set_xlabel('Epoch', fontsize=12)\n",
    "ax.set_ylabel('Test Accuracy', fontsize=12)\n",
    "ax.set_title('Comparaison des méthodes d\\'accélération sur MNIST', fontsize=14)\n",
    "ax.legend(loc='lower right')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim(0.9, 1.0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('acceleration_comparison_1_2_3.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e26fab",
   "metadata": {},
   "source": [
    "## 12. Conclusion et Discussion\n",
    "\n",
    "### Méthodes d'accélération explorées :\n",
    "\n",
    "1. **Optimizers adaptatifs (Adam vs SGD)**\n",
    "   - Source : Deep Learning Book, Chapitre 8\n",
    "   - Adam converge généralement plus vite que SGD grâce à l'adaptation du learning rate\n",
    "   - Moins sensible au choix initial du learning rate\n",
    "\n",
    "2. **Batch Normalization**\n",
    "   - Normalise les activations entre les couches\n",
    "   - Permet d'utiliser des learning rates plus élevés\n",
    "   - Régularise implicitement le modèle\n",
    "\n",
    "3. **Learning Rate Scheduling**\n",
    "   - Diminuer le LR au cours de l'entraînement permet une convergence plus fine\n",
    "   - StepLR, ExponentialLR, CosineAnnealing sont des stratégies courantes\n",
    "\n",
    "4. **Taille de batch**\n",
    "   - Grands batches : meilleure utilisation GPU, moins de bruit\n",
    "   - Petits batches : convergence parfois plus rapide en epochs, effet régularisant\n",
    "\n",
    "### Analyse des bottlenecks :\n",
    "- Sur CPU : le temps est dominé par les convolutions\n",
    "- Sur GPU : le temps peut être dominé par les transferts mémoire pour de petits modèles\n",
    "\n",
    "### Recommandation finale :\n",
    "Pour atteindre 97% le plus rapidement sur MNIST :\n",
    "- Utiliser **Adam** comme optimizer\n",
    "- Ajouter **Batch Normalization** au réseau\n",
    "- Batch size de **128-256** (bon compromis)\n",
    "- Learning rate initial de **0.001-0.002**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
